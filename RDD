RDD - Relisient Distributed Dataset

Resilient: Restore the data on failure.
Distributed: Data is distributed among different nodes.
Dataset: Group of data.

Defination:-RDD (Resilient Distributed Dataset) is a fundamental data structure in Apache Spark that represents a collection of objects distributed across a cluster.
RDDs are designed to be fault-tolerant, immutable, and can be processed in parallel, making them suitable for big data applications.


Features of RDD

Resilience: RDDs are fault-tolerant. If a partition of an RDD is lost, it can be recomputed using the lineage of transformations that created it.

Distributed: RDDs are distributed across the nodes of a cluster, allowing for parallel processing of data.

Immutability: Once an RDD is created, it cannot be changed. However, you can create new RDDs from existing ones through transformations.

In-memory Computation: RDDs can be cached in memory, enabling faster access and computation for iterative algorithms.

Lazy Evaluation: Transformations on RDDs are evaluated lazily. The actual computation is deferred until an action is called, allowing Spark to optimize the execution plan.

Transformations:
Transformations are operations that create a new RDD from an existing one. They are lazy, meaning they do not compute their results immediately.
Instead, they build up a lineage of transformations to be executed when an action is called.

Actions:
Actions are operations that trigger the execution of transformations and return a result to the driver program or write data to an external storage system.
They materialize the RDD and are typically where the computation occurs.

Spark Caching
persist()
Cache()
unpersist()

Both are Same with persist we can save in different location based on size of the dataframe.
1.MEMORY_ONLY          --Memory-Only Storage
2.MEMORY_AND_DISK      --Memory and Disk Storage
3.DISK_ONLY            --Disk only
4.MEMORY_ONLY_SER      -- Memory-Only Serialized
5.MEMORY_AND_DISK_SER  --MEMORY_AND_DISK_SER
6.MEMORY_ONLY_SER      --Off-Heap Storage

print("is dataframe cached",is_cached) : we can check the whether dataframe is cached or not if yes it will return "True" else "False"

Example:-
df.persist(StorageLevel.MEMORY_AND_DISK)
df1.cache()
df.unpersist()

Broadcast Variables:
In PySpark RDD and DataFrame, Broadcast variables are read-only shared variables that are cached and available on all nodes in a cluster in-order to access or use by the tasks.
Instead of sending this data along with every task,PySpark distributes broadcast variables to the workers using efficient broadcast algorithms to reduce communication costs.

https://sparkbyexamples.com/pyspark/pyspark-broadcast-variables/ for my reference

Joins
nner Join: Combines matching keys from both RDDs.
Left Outer Join: Includes all keys from the left RDD and matched keys from the right RDD.
Right Outer Join: Includes all keys from the right RDD and matched keys from the left RDD.
Full Outer Join: Includes all keys from both RDDs, with None for unmatched keys.


RDD, Dataframe, Dataset [Difference and Converting from one to
Another]:
• Which is faster & why
• Differences Between RDD, Dataframe, and Dataset
• Converting between RDD, Dataframe, and Dataset

--RDD to Dataframe
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("example").getOrCreate()
df = rdd.toDF(["column1", "column2"])  # Specify column names

--Dataframe to RDD
rdd = df.rdd

--From DataFrame to Dataset (in Scala)
val ds = df.as[YourCaseClass]

--From Dataset to DataFrame
val df = ds.toDF()

Which is Faster & Why
DataFrame and Dataset: Generally faster than RDDs due to
1.Catalyst Optimizer: Optimizes query plans for better performance.
2.Tungsten Execution Engine: Provides memory management and code generation for faster execution.

RDD: Slower because it lacks these optimizations and does not leverage Spark's execution engine fully.


RDDs are lower-level and less optimized, suitable for unstructured data.
DataFrames provide a higher-level, optimized API for structured data.
Datasets combine both, offering compile-time type safety and optimization.

Transformations are two types
1.Narrow Transformations(Do not shuffle data)
--Narrow transformations are transformations where each partition of the parent RDD is used to compute at most one partition of the child RDD.
In other words, there is a one-to-one relationship between the partitions of the parent and child.
--They do not require data to be shuffled across the cluster.

example:
-filter()
-Map()
-union()

2.Wide Transformations
Wide transformations are transformations where each partition of the parent RDD can contribute to multiple partitions of the child RDD.
This involves shuffling data across partitions, which can be more costly in terms of performance.
-They often require data to be redistributed across the network.

Example:
-Join()
-distinct()
-groupByKey()
-reducedByKey()

• Understanding Shuffling in Spark
Shuffling: Shuffling is the process of redistributing data across partitions. It occurs during wide transformations and is one of the most expensive operations in Spark.
Causes of Shuffling:
When performing operations that require data from multiple partitions to be combined or aggregated.
Operations like groupBy, reduceBy, and join trigger shuffling.
Impact on Performance:
Shuffling can lead to increased latency and reduced performance due to network I/O, disk I/O, and increased memory usage.

